#!/bin/bash
####################################################################################################################################################################
# UTILITY FUNCTIONS
####################################################################################################################################################################
secho() { # The "echo" builtin should not be used with arbitrary strings since it does not support termination of the parameter list
    printf '%s\n' "$*"
}

stdout() {
    secho "$@" >&1
}

stderr() {
    secho "$@" >&2
}

die() {
    stderr 'ERROR:' "$@"
    exit 1
}

err_handler() {
    die "error at line $1, last exit code is $2" 
}
####################################################################################################################################################################


####################################################################################################################################################################
# SHELL PARAMETERS
####################################################################################################################################################################
set -o nounset
set -o pipefail
set -o errexit
set -o errtrace
shopt -s nullglob
shopt -s failglob
trap 'err_handler "$LINENO" "$?"' ERR
####################################################################################################################################################################

####################################################################################################################################################################
# CONFIG
####################################################################################################################################################################
JAVA_HOME=/usr/lib/jvm/jre-1.7.0-openjdk.x86_64
MOGOOP_DIR=~/mogoop
NO_DEDICATED_MASTER_HOST=1 # Don't use a separate machine for Jobtracker/Namenode
PER_NODE_MAPPER_CORES=60	# TODO: Auto-config per machine. The HOSTS array already stores CPU count for each.
PER_NODE_REDUCER_CORES=60	# TODO: Auto-config per machine. The HOSTS array already stores CPU count for each.

HADOOP_INSTALL="$MOGOOP_DIR/hadoop"
HADOOP_MASTER_LIST="$HADOOP_DIR/conf/masters"
HADOOP_SLAVE_LIST="$HADOOP_DIR/conf/slaves"
HADOOP_CORE_SITE_XML="$HADOOP_DIR/conf/core-site.xml"
HADOOP_CORE_SITE_XML_TEMPLATE="./templates/hadoop-1.2.1/conf/core-site.xml"
HADOOP_HDFS_SITE_XML="$HADOOP_DIR/conf/hdfs-site.xml"
HADOOP_HDFS_SITE_XML_TEMPLATE="./templates/hadoop-1.2.1/conf/hdfs-site.xml"
HADOOP_MAPRED_SITE_XML="$HADOOP_DIR/conf/mapred-site.xml"
HADOOP_MAPRED_SITE_XML_TEMPLATE="./templates/hadoop-1.2.1/conf/mapred-site.xml"
HADOOP_ENV_SH="$HADOOP_DIR/conf/hadoop-env.sh"
HADOOP_ENV_SH_TEMPLATE="./templates/hadoop-1.2.1/conf/hadoop-env.sh"
####################################################################################################################################################################

####################################################################################################################################################################
# GLOBALS
####################################################################################################################################################################
declare -A HOSTS	# key = hostname, value = cpu count
HOST_SELF=
####################################################################################################################################################################


####################################################################################################################################################################
# THE ACTUAL CODE
####################################################################################################################################################################

# Schedules main_node() to be run via LSF' bsub. Takes care of proper memory settings
main() {
	if [ "$#" -lt 1 ] || [[ "$1" != *[0-9]* ]] ; then
		die "Syntax: $0 number_of_cluster_nodes"
	fi
	
	local node_count="$1"
	shift
	
	# Each Mogon node has 64 CPUs and 128 GiB of RAM. We reserve all CPUs by stating that we run 64 processes.
	# The Reserve1800M profile will reserve 1800 MiB per process (yes, MiB, not MB). 
	# Multiplied by our 64 processes we get almost all of the memory and leave some of it for the operating system.
	# We need to allow the processes to actually use that limit with "-M": It sets the per-process memory limit.
	# To be graceful, we allow four times the 1800 MiB per process in case some instances need a bit more.
	bsub -I -n $(($node_count*64)) -R 'span[ptile=64]' -app Reserve1800M -M $((4*1800*1024)) -q short "$0" "RUN_MAIN_NODE" "$@"
}

# Run via LSF on the master node. Starts namenode, datanodes, jobtracker, etc. Then opens a shell so the user can use hadoop.
main_node() {
	get_hosts

	stdout "Jobtracker / Namenode: $SELF"

	# If configured, don't dedicate the whole local machine as Jobtracker/Namenode.
	# It typically has 64 cores at Mogon which is quite a lot.
	if [ "$NO_DEDICATED_MASTER_HOST" -eq 0 ] ; then
		remove_host "$SELF"
	fi

	# Truncate masters file:
	# The local machine is automatically selected as master.
	# The purpose of the masters-list would be to add SECONDARY namenodes / jobtrackers - which we don't want.
	> "$HADOOP_MASTER_LIST"
	stdout "Secondary namenodes:" "$(<"$HADOOP_MASTER_LIST")"

	# Set all remaining hosts as slave
	stdout "Worker machines: ${!HOSTS[@]}"
	> "$HADOOP_SLAVE_LIST"
	for host in "${!HOSTS[@]}" ; do
		stdout "$host" >> "$HADOOP_SLAVE_LIST"
	done

	# See https://mogon.zdv.uni-mainz.de/dokuwiki/local_scratch
	local LSF_JOB_TEMP_DIR="/jobdir/$LSB_JOBID"

	local NAMENODE_DATA_DIR="$LSF_JOB_TEMP_DIR/namenode-data"
	local DATANODE_DATA_DIR="$LSF_JOB_TEMP_DIR/datanode-data"
	local MAPRED_DATA_DIR="$LSF_JOB_TEMP_DIR/mapred-data"
    
	# core-site.xml: Set ourself as namenode
	replace_string_in_file 'MOGOOP_NAMENODE' "$SELF" "$HADOOP_CORE_SITE_XML_TEMPLATE" > "$HADOOP_CORE_SITE_XML"
	
	# hdfs-site.xml: Set namenode / datanode data dir
	replace_string_in_file 'MOGOOP_NAMENODE_DATA_DIR' "$NAMENODE_DATA_DIR" "$HADOOP_HDFS_SITE_XML_TEMPLATE" > "$HADOOP_HDFS_SITE_XML.temp"
	replace_string_in_file 'MOGOOP_DATANODE_DATA_DIR' "$DATANODE_DATA_DIR" "$HADOOP_HDFS_SITE_XML.temp" > "$HADOOP_HDFS_SITE_XML"
	
	# mapred-site.xml: Set ourself as jobtracker, data dir for mappers/reducers, and number of mapper/reducer cores per node.
	replace_string_in_file 'MOGOOP_JOBTRACKER' "$SELF" "$HADOOP_MAPRED_SITE_XML_TEMPLATE" > "$HADOOP_MAPRED_SITE_XML.temp1"
	replace_string_in_file 'MOGOOP_MAPRED_DATA_DIR' "$MAPRED_DATA_DIR" "$HADOOP_MAPRED_SITE_XML.temp1" > "$HADOOP_MAPRED_SITE_XML.temp2"
	replace_string_in_file 'MOGOOP_PER_NODE_MAPPER_CORES' "$PER_NODE_MAPPER_CORES" "$HADOOP_MAPRED_SITE_XML.temp2" > "$HADOOP_MAPRED_SITE_XML.temp3"
	replace_string_in_file 'MOGOOP_PER_NODE_REDUCER_CORES' "$PER_NODE_REDUCER_CORES" "$HADOOP_MAPRED_SITE_XML.temp3" > "$HADOOP_MAPRED_SITE_XML"
	
	# hadoop-env.sh: Set JAVA_HOME. As of 2013-11-03, I tried using "export JAVA_HOME" but Hadoop didn't eat it.
	replace_string_in_file 'MOGOOP_JAVA_HOME' "$JAVA_HOME" "$HADOOP_ENV_SH_TEMPLATE" > "$HADOOP_ENV_SH"

	export HADOOP_INSTALL

	"$HADOOP_DIR/bin/start-dfs.sh"
	"$HADOOP_DIR/bin/start-mapred.sh"

	stdout ""
	stdout "Hadoop cluster running. Opening shell so you can use it."
	stdout "Exit the shell to terminate the Hadoop cluster."

	PATH="$PATH:$HADOOP_DIR/bin" bash -i || true

	"$HADOOP_DIR/bin/stop-mapred.sh"
	"$HADOOP_DIR/bin/stop-dfs.sh"
}

get_hosts() {
	local -a hosts_temp
	# Get all hosts of the LSF job. Its a list of pairs: First item = hostname, second = cpu count
	read -ra hosts_temp <<< "$LSB_MCPU_HOSTS"

	local i=0
	for ((i=0; i < ${#hosts_temp[@]}; i+=2)) ; do
		local host="${hosts_temp[$i]}"
		local cpu_count="${hosts_temp[$(($i+1))]}"
		HOSTS[$host]=$cpu_count
		stdout "Host $host: $cpu_count cpus"
	done

	# Get host where this script is running
	SELF="$(hostname --short)" # --short removes the domain name, which get_hosts_sorted won't include
}

remove_host() {
	unset "HOSTS[$1]"
}

# Inspired by http://mywiki.wooledge.org/BashFAQ/021
# $1 = to replace (bash matching pattern)
# $2 = replacement
# $3 = input file
replace_string_in_file() {
	while IFS= read -r line; do
		stdout "${line//"$1"/$2}"
	done < "$3"
}

if [ "$1" = RUN_MAIN_NODE ] ; then
	shift
	main_node "$@"
else
	main "$@"
fi
####################################################################################################################################################################
